%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{babel}
\usepackage{bm}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Pp}{\mathbb{P}}

\begin{document}

\section{Optimization Problem}
Problem is
\begin{align}
    \min_{\vec{x} \in X} f_0(\vec{x}), \\
    f_i(\vec{x}) \leq 0,
\end{align}
for $i = 1, \dots, m$, where $X$ is a box:
\begin{equation}
    \{\vec{x} \in \mathbb{R}^n \mid x_j^{\text{min}} \leq x_j \leq x_j^{\text{max}}\}.
\end{equation}

\section{Approximate Problem}

Suppose $k$th iterate is $\vec{x}^{(k)}$. Then, for $i = 0, \dots, m$, replace $f_i(\vec{x})$ with
\begin{equation}
    g_i(\vec{x}) = f_i(\vec{x}^{(k)}) + \nabla f_i(\vec{x}^{(k)}) \cdot (\vec{x} - \vec{x}^{(k)}) + \frac{\rho_i^2}{2} \left|\frac{\vec{x} - \vec{x}^{(k)}}{\vec{\sigma}}\right|^2,
\end{equation}

where $\vec{\sigma}$ and $\vec{\rho}$ are vectors and the division by $\vec{\sigma}$ is element-wise. And make a trust region
\begin{equation}
    T = \{\vec{x} \mid |x_j - x_j^{(k)}| \leq \sigma_j\} \cap X.
\end{equation}

So that the new problem is
\begin{align}
    \min_{x \in T} g_0(\vec{x}), \\
    g_i(\vec{x}) \leq 0.
\end{align}

\section{Overall Scheme}

For the $k$th iteration, with current candidate $\vec{x}^{(k)}$:
\begin{enumerate}
    \item Solve approximate problem to find candidate $\vec{x}^{(k+1)}$
    \item Check conservative: $g_i(\vec{x}^{(k+1)}) < f_i(\vec{x}^{(k+1)})$.
    \begin{itemize}
        \item If no, throw away candidate, double $\rho_i$ for each non-conservative $g_i$, and solve approximate problem again.
    \end{itemize}
    \item Halve $\vec{\rho}$ (take bigger steps) and update $\vec{\sigma}$ (decrease $\sigma_i$ if $x_i$ oscillating, increase if monotonic i.e. heading somewhere else).
\end{enumerate}

\section{Solving approximate problem}

\subsection{Evaluating dual function}
Lagrangian relaxation solves a minimization problem over the space
\begin{equation}
   \Lambda = \{\vec{\lambda} \in \mathbb{R}^{m+1} | \lambda_0 = 1, \lambda \geq 0\},
\end{equation}
with the objective
\begin{align}
    L(x, \vec{\lambda}) &= \sum_{i=0}^m \lambda_i g_i(\vec{x}) \\ 
                  &= \sum_{i=0}^m \lambda_i f_i(\vec{x}^{(k)}) + \left(\sum_{i=0}^m \lambda_i \nabla f_i(\vec{x}^{(k)})\right) \cdot (\vec{x} - \vec{x}^{(k)})\\
                  &\quad + \frac{1}{2} \left(\sum_{i=0}^m \lambda_i \rho_i\right) \left|\frac{\vec{x}-\vec{x}^{(k)}}{\vec{\sigma}}\right|^2 \\
                  &= \vec{\lambda} \cdot \vec{f}(\vec{x}^{(k)}) + \sum_{j=1}^n h_j(x_j - x_j^{(k)}),
\end{align}
where
\begin{equation}
    h_j(\delta_j) =  \left(\vec{\lambda} \cdot \nabla f(x_0)_j  \right)\delta_j + \frac{1}{2\sigma_j} (\vec{\lambda} \cdot \vec{\rho}) \delta_j^2.
\end{equation}
Define dual function,
\begin{align}
    g(\vec{\lambda}) &= \min_{x \in T} L(x, \vec{\lambda}) \\
               &= \vec{\lambda} \cdot \vec{f}(\vec{x}^{(k)}) + \sum_{j=1}^n \left(\min_{|\delta_j| \leq \sigma_j}  h_j(\delta_j)\right).
\end{align}
%To evaluate, analytically minimize quadratic in $\delta_j = x_j - x_j^{(k)}$, snapping to bounds. 
Define for each $j$
\begin{align}
    a_j &= \frac{1}{2\sigma_j^2} (\vec{\lambda} \cdot \vec{\rho}) 
    \label{eq:a} \\
    b_j &= \vec{\lambda} \cdot \nabla f(x_0)_j
    \label{eq:b}
\end{align}
Note that we can write
    \begin{equation}
        b = \nabla f(x_0)^T \vec{\lambda}.
    \end{equation}
    We now have,
\begin{equation}
    h_j(\delta_j) = b_j \delta_j + a_j \delta_j^2.
\end{equation}
The minimum of $h_j(\delta_j)$ is found at
    \begin{equation}
        \delta_j^* = -\frac{b_j}{2a_j}\text{ clamped to } [-\sigma_j, \sigma_j].
    \end{equation}
And hence we can determine 
    \begin{equation}
        g(\vec{\lambda}) = \vec{\lambda} \cdot \vec{f}(\vec{x}^{(k)}) + \sum_{j=1}^n \left(b_j \delta_j^* + a_j (\delta_j^*)^2\right).
    \end{equation}
    Now let us compute the gradient. Note that,
\begin{align}
    \frac{\partial a_j}{\partial \lambda_i} &= \frac{\rho_i}{2\sigma_j^2}, \\
    \frac{\partial b_j}{\partial \lambda_i} &= \nabla f_i(\vec{x}^{(k)})_j.
\end{align}
Let $S \subseteq \{1,\dots, n\}.$ be the indices where we don't snap to bounds. Then,
\begin{align}
    \frac{\partial g}{\partial \lambda_i} &= f_i(x^{(k)}) + \sum_{j=1}^n \left(\frac{\partial b_j}{\partial \lambda_i} \delta_j^*  + \frac{\partial a_j}{\partial \lambda_i} (\delta_j^*)^2 \right) \\
         &\quad + \sum_{j \in S} (b_j + 2a_j \delta^*_j) \frac{\partial \delta^*_j}{\partial \lambda_i} \\
         &= f_i(x^{(k)}) + \nabla f_i(\vec{x}^{(k)}) \delta^*  + \rho_i \sum_{j=1}^n \frac{(\delta_j^*)^2}{2 \sigma_j^2}.
    \label{eq:dg}
\end{align}
In vectorial form,
    \begin{equation}
        \frac{\partial g}{\partial \lambda} = f_i(x^{(k)}) + \nabla f(\vec{x}^{(k)}) \delta^* + \rho \sum_{j=1}^n \frac{(\delta_j^*)^2}{2 \sigma_j^2}.
    \end{equation}
%We should evaluate $a_j$ and $b_j$ for each $j$, taking advantage of sparsity. Note that $\frac{\partial b_j}{\partial y_i}$ is also sparse. But $\frac{\partial a_j}{\partial y_i}$ is not?

%Explicitltly, the first term of Equation (\ref{eq:dgdy}) has the sparsity pattern of the Jacobian transposed, while the second term is low-rank: $\frac{1}{2} \cdot$ the outer product of $\frac{1}{\vec{\sigma}}$ and $\vec{\rho}$.

\subsection{Maximizing dual function}

The dual problem is,
\begin{equation}
    \max_{y \geq 0} g(\lambda).
\end{equation}
We can provide $g$ and its gradient function recursively to CCSA, which will solve it for us.

\section{Main Goals}

\begin{itemize}
    \item Support sparse Jacobians
    \item Support affine constraints
        \begin{itemize}
            \item Does paper handle these? (First, find where paper handles box constraints.)
            \item Maybe think of this as a more complicated $X$, rather than a simple $f_i$.
        \end{itemize}
\end{itemize}

\end{document}
